# Adversarial-Patch-Defence
## Problem Statement
A majority of prior work has focused on attacking with and defending against either small or imperceptible changes to the input. Recent work by Brown et al. [1] and Karmon et al. [2] have studied - adversarial examples under a new threat model- an attacker that crafts perturbations that are not bounded by an $\epsilon$ value but are bounded to a small region or location in the image. Therefore, a form of localised and visible contiguous perturbation of image pixels emerged, known as Patch Attacks. Simultaneously. the detection and defence of practical patch attacks have gained the utmost popularity in the community concerning the security threat of CNNs. However, how to efficiently defend against this attack is still an open question. In this project, we would like you to focus on the project related to adversarial patch defense:


## Project Goals
- Adversarial Patch 

## Code

## Reference
[1] Brown T B, Man√© D, Roy A, et al. Adversarial patch[J]. arXiv preprint arXiv:1712.09665, 2017
[2] Karmon D, Zoran D, Goldberg Y. Lavan: Localized and visible adversarial noise[C]//International Conference on Machine Learning. PMLR, 2018: 2507-2515.
